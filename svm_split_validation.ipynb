{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --extra-index-url=https://pypi.nvidia.com cudf-cu11 cuml-cu11\n",
        "!pip install pandas==1.5.3"
      ],
      "metadata": {
        "id": "aR7AjCmNDPPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \\\n",
        "    cuml-cu12 --extra-index-url=https://pypi.nvidia.com \\\n",
        "    cudf-cu12 --extra-index-url=https://pypi.nvidia.com \\\n",
        "    rmm-cu12 --extra-index-url=https://pypi.nvidia.com \\\n",
        "    rapids-dask-dependency --extra-index-url=https://pypi.nvidia.com\n",
        "\n",
        "print(\"Cài đặt lại RAPIDS cho CUDA 12 hoàn tất!\")"
      ],
      "metadata": {
        "id": "OzlXMMdc6Xyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtH6Hd29W8b2"
      },
      "outputs": [],
      "source": [
        "\n",
        "#split\n",
        "import os\n",
        "from abc import ABCMeta, abstractmethod\n",
        "from numbers import Integral, Real\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from scipy.special import logsumexp\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "import pickle\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "\n",
        "try:\n",
        "    import cuml\n",
        "    from cuml.svm import SVC as CuSVC\n",
        "    print(\"CuML imported successfully - GPU support enabled!\")\n",
        "    GPU_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"CuML not available, falling back to CPU sklearn\")\n",
        "    from sklearn.svm import SVC as CuSVC\n",
        "    GPU_AVAILABLE = False\n",
        "\n",
        "print(\"Checking GPU availability...\")\n",
        "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader,nounits\n",
        "\n",
        "def main(folder_path):\n",
        "    with open(\"log_result_SVM_split_validation.txt\", \"a\") as log_file:\n",
        "        for file_name in os.listdir(folder_path):\n",
        "            if file_name.endswith(\".csv\"):\n",
        "                file_path = os.path.join(folder_path, file_name)\n",
        "                print(f\"Processing file: {file_name}\")\n",
        "\n",
        "                cols = [\"Destination Port\", \"Flow Duration\", \"Total Fwd Packets\", \"Total Backward Packets\",\n",
        "                        \"Total Length of Fwd Packets\", \"Total Length of Bwd Packets\", \"Fwd Packet Length Max\",\n",
        "                        \"Fwd Packet Length Min\", \"Fwd Packet Length Mean\", \"Fwd Packet Length Std\",\n",
        "                        \"Bwd Packet Length Max\", \"Bwd Packet Length Min\", \"Bwd Packet Length Mean\",\n",
        "                        \"Bwd Packet Length Std\", \"Flow Bytes/s\", \"Flow Packets/s\", \"Flow IAT Mean\",\n",
        "                        \"Flow IAT Std\", \"Flow IAT Max\", \"Flow IAT Min\", \"Fwd IAT Total\", \"Fwd IAT Mean\",\n",
        "                        \"Fwd IAT Std\", \"Fwd IAT Max\", \"Fwd IAT Min\", \"Bwd IAT Total\", \"Bwd IAT Mean\",\n",
        "                        \"Bwd IAT Std\", \"Bwd IAT Max\", \"Bwd IAT Min\", \"Fwd PSH Flags\", \"Bwd PSH Flags\",\n",
        "                        \"Fwd URG Flags\", \"Bwd URG Flags\", \"Fwd Header Length\", \"Bwd Header Length\",\n",
        "                        \"Fwd Packets/s\", \"Bwd Packets/s\", \"Min Packet Length\", \"Max Packet Length\",\n",
        "                        \"Packet Length Mean\", \"Packet Length Std\", \"Packet Length Variance\", \"FIN Flag Count\",\n",
        "                        \"SYN Flag Count\", \"RST Flag Count\", \"PSH Flag Count\", \"ACK Flag Count\", \"URG Flag Count\",\n",
        "                        \"CWE Flag Count\", \"ECE Flag Count\", \"Down/Up Ratio\", \"Average Packet Size\",\n",
        "                        \"Avg Fwd Segment Size\", \"Avg Bwd Segment Size\", \"Fwd Avg Bytes/Bulk\",\n",
        "                        \"Fwd Avg Packets/Bulk\", \"Fwd Avg Bulk Rate\", \"Bwd Avg Bytes/Bulk\", \"Bwd Avg Packets/Bulk\",\n",
        "                        \"Bwd Avg Bulk Rate\", \"Subflow Fwd Packets\", \"Subflow Fwd Bytes\", \"Subflow Bwd Packets\",\n",
        "                        \"Subflow Bwd Bytes\", \"Init_Win_bytes_forward\", \"Init_Win_bytes_backward\",\n",
        "                        \"act_data_pkt_fwd\", \"min_seg_size_forward\", \"Active Mean\", \"Active Std\",\n",
        "                        \"Active Max\", \"Active Min\", \"Idle Mean\", \"Idle Std\", \"Idle Max\", \"Idle Min\", \"Label\"]\n",
        "\n",
        "                df = pd.read_csv(file_path, names=cols)\n",
        "                df[\"Label\"] = df[\"Label\"].apply(lambda x: 0 if x == \"BENIGN\" else 1)\n",
        "                print(df[\"Label\"].value_counts())\n",
        "                print(df.head())\n",
        "\n",
        "                # Split data\n",
        "                train_df, test_df = train_test_split(df, test_size=0.2, stratify=df[\"Label\"], random_state=42)\n",
        "                train_df, valid_df = train_test_split(train_df, test_size=0.25, stratify=train_df[\"Label\"], random_state=42)\n",
        "\n",
        "                # Preprocess data\n",
        "                for subset in [train_df, valid_df, test_df]:\n",
        "                    for col in subset.columns[:-1]:\n",
        "                        subset[col] = pd.to_numeric(subset[col], errors='coerce')\n",
        "                    subset.dropna(inplace=True)\n",
        "\n",
        "                x_train = train_df[train_df.columns[:-1]].values\n",
        "                y_train = train_df[train_df.columns[-1]].values\n",
        "                x_valid = valid_df[valid_df.columns[:-1]].values\n",
        "                y_valid = valid_df[valid_df.columns[-1]].values\n",
        "                x_test = test_df[test_df.columns[:-1]].values\n",
        "                y_test = test_df[test_df.columns[-1]].values\n",
        "\n",
        "                x_train = np.nan_to_num(x_train, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "                x_valid = np.nan_to_num(x_valid, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "                x_test = np.nan_to_num(x_test, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "                # Standardize features\n",
        "                scaler = StandardScaler()\n",
        "                x_train = scaler.fit_transform(x_train)\n",
        "                x_valid = scaler.transform(x_valid)\n",
        "                x_test = scaler.transform(x_test)\n",
        "\n",
        "                print(\"Training model...\")\n",
        "                svm_model = CuSVC(\n",
        "                      C=1.0,\n",
        "                      kernel='rbf',\n",
        "                      gamma='scale',\n",
        "                      tol=1e-3,\n",
        "                      cache_size=500.0,\n",
        "                      max_iter=-1\n",
        "                    )\n",
        "\n",
        "\n",
        "                y_pred = svm_model.predict(x_test)\n",
        "\n",
        "                log_file.write(f\"Result of file: {file_name}\\n\")\n",
        "                log_file.write(f\"Library: {'CuML (GPU)' if GPU_AVAILABLE else 'sklearn (CPU)'}\\n\")\n",
        "                log_file.write(classification_report(y_test, y_pred))\n",
        "                log_file.write(f\"Accuracy: {accuracy_score(y_test, y_pred)*100:.2f}%\\n\")\n",
        "                log_file.write(\"Model training complete.\\n\")\n",
        "                log_file.write(\"---------------------------------------------------\\n\\n\")\n",
        "\n",
        "                print(classification_report(y_test, y_pred))\n",
        "                print(f\"Result of file: {file_name}\")\n",
        "                print(f\"Library: {'CuML (GPU)' if GPU_AVAILABLE else 'sklearn (CPU)'}\")\n",
        "                print(f\"Accuracy: {accuracy_score(y_test, y_pred)*100:.2f}%\")\n",
        "                print(\"Model training complete.\")\n",
        "                print(\"---------------------------------------------------\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    folder_path = \"/content/final_dts\"\n",
        "    main(folder_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}